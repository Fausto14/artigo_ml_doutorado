{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../Util'))\n",
    "from dados import ProcessarDados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "procData = ProcessarDados(\"../dataset/norm_bin_10_FEATURES_M17_CM6b_TH199.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcção que treina e testa o modelo armazenando as métricas\n",
    "# retorna um dicionário cotendo os valores das métricas de cada rodada\n",
    "def treinamento_teste(epocas = 10, k_folds = 5, exibir_matriz_confusao=False, exibir_metricas=False):\n",
    "    \n",
    "    #array para armazenar as das métricas de cada rodada\n",
    "    resultados_accuracy = []\n",
    "    resultados_precision = []\n",
    "    resultados_recall = []\n",
    "    resultados_f1 = []\n",
    "    resultados_parametros = []\n",
    "    \n",
    "    #dicionário das métricas\n",
    "    resultados_gerais = {}\n",
    "\n",
    "    for i in range(epocas):\n",
    "        # divisão os dados \n",
    "        X_train, X_test, y_train, y_test = procData.holdout(0.2)\n",
    "        #print(Counter(y_test))\n",
    "\n",
    "        # realizando o grid search para encontrar a melhor combinação entre o C, gamma e Kernel, \n",
    "        # considerando a acurácia (taxa de acerto)\n",
    "        # aqui o método GridSearchCV é configurado para subdividir os dados de treino em k_folds\n",
    "        \n",
    "        \n",
    "        clf = GradientBoostingClassifier(random_state=None)\n",
    "        grid_gb = GridSearchCV(clf, param_grid, cv=k_folds, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "        grid_gb.fit(X_train, y_train)\n",
    "\n",
    "        # Treinando do modelo com os melhores parametros encontrados\n",
    "        #n_estimators_best = grid_gb.best_estimator_.n_estimators\n",
    "        #learning_rate_best = grid_gb.best_estimator_.learning_rate\n",
    "        min_samples_best = grid_gb.best_estimator_.min_samples_leaf\n",
    "        max_depth_best = grid_gb.best_estimator_.max_depth\n",
    "\n",
    "        #GB = GradientBoostingClassifier(random_state = None, n_estimators = n_estimators_best, learning_rate =  learning_rate_best, max_depth = max_depth_best)\n",
    "        \n",
    "        GB = GradientBoostingClassifier(random_state = None, max_depth = max_depth_best, min_samples_leaf=min_samples_best)\n",
    "        GB.fit(X_train, y_train)\n",
    "\n",
    "        #testando o modelo\n",
    "        y_pred = GB.predict(X_test)\n",
    "        if exibir_matriz_confusao:\n",
    "            print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # calculado as metricas\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        precision = metrics.precision_score(y_test, y_pred)\n",
    "        recall = metrics.recall_score(y_test, y_pred)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # armazenando as métricas\n",
    "        resultados_accuracy.append(accuracy)\n",
    "        resultados_precision.append(precision)\n",
    "        resultados_recall.append(recall)\n",
    "        resultados_f1.append(f1_score)\n",
    "\n",
    "        #best_parametros = \"n_estimators: \"+ str(n_estimators_best)+ \" | learning_rate: \"+ str(learning_rate_best)+ \" | max_depth: \"+ str(max_depth_best);\n",
    "        best_parametros = \" max_depth: \"+ str(max_depth_best) +\"| min_samples_leaf:\"+ str(min_samples_best);\n",
    "        resultados_parametros.append(best_parametros)\n",
    "\n",
    "\n",
    "        if exibir_metricas:\n",
    "            print(\"Rodada: #\",i)\n",
    "            print(best_parametros)\n",
    "            print(\"Accuracy:\",accuracy)\n",
    "            print(\"Precision:\",precision)\n",
    "            print(\"Recall:\",recall)\n",
    "            print(\"F1-Score:\",f1_score)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            \n",
    "    resultados_gerais['accuracy'] = resultados_accuracy\n",
    "    resultados_gerais['precision'] = resultados_precision\n",
    "    resultados_gerais['recall'] = resultados_recall\n",
    "    resultados_gerais['f1'] = resultados_f1\n",
    "    \n",
    "    return resultados_gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabelaMetricas(nome_modelo, dict_metricas, rodadas=False, salvarResultados=True):\n",
    "        \n",
    "    print (\"============================================== \"+nome_modelo+\" =================================================\")\n",
    "    print (\"=================================== TABELA DE MÉTRICAS DO MODELO ===================================\")\n",
    "    \n",
    "    if(rodadas==False):\n",
    "        print (\"\\t Accuracy \\t|\\t Precision \\t|\\t Recall \\t|\\t F1-Score\")\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['accuracy'], axis=0), np.std(dict_metricas['accuracy'], axis=0)),end='       ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['precision'], axis=0), np.std(dict_metricas['precision'], axis=0)),end='    ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['recall'], axis=0), np.std(dict_metricas['recall'], axis=0)),end='       ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['f1'], axis=0), np.std(dict_metricas['f1'], axis=0)))\n",
    "        print (\"====================================================================================================\")\n",
    "        \n",
    "    if(salvarResultados):\n",
    "        # save to npy file\n",
    "        np.save('../resultados/resultados_'+nome_modelo+'.npy', dict_metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo os parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators  = [100, 110, 120]\n",
    "#learning_rate = [0.1, 0.3, 0.5, 1.0]\n",
    "max_depth  = np.arange(2,9,2)\n",
    "min_samples_leaf = np.arange(1,10,2)\n",
    "#max_depth = [3]\n",
    "#param_grid = {'n_estimators': n_estimators, 'learning_rate' : learning_rate, 'max_depth': max_depth}\n",
    "param_grid = {'max_depth': max_depth, 'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "epocas = 50\n",
    "k_folds = 5\n",
    "exibir_matriz_confusao = True\n",
    "exibir_metricas = True\n",
    "salvarResultados = True\n",
    "rodadas=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando e obtendo as métricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  4]\n",
      " [ 4 90]]\n",
      "Rodada: # 0\n",
      " max_depth: 8| min_samples_leaf:3\n",
      "Accuracy: 0.9333333333333333\n",
      "Precision: 0.9574468085106383\n",
      "Recall: 0.9574468085106383\n",
      "F1-Score: 0.9574468085106385\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# treinando o modelo\n",
    "dict_metricas = treinamento_teste(epocas, k_folds, exibir_matriz_confusao, exibir_metricas)\n",
    "tabelaMetricas('GBDT',dict_metricas, rodadas, salvarResultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
