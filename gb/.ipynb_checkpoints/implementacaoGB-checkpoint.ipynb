{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../Util'))\n",
    "from dados import ProcessarDados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "procData = ProcessarDados(\"../dataset/norm_bin_10_FEATURES_M17_CM6b_TH199.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcção que treina e testa o modelo armazenando as métricas\n",
    "# retorna um dicionário cotendo os valores das métricas de cada rodada\n",
    "def treinamento_teste(epocas = 10, k_folds = 5, exibir_matriz_confusao=False, exibir_metricas=False):\n",
    "    \n",
    "    #array para armazenar as das métricas de cada rodada\n",
    "    resultados_accuracy = []\n",
    "    resultados_precision = []\n",
    "    resultados_recall = []\n",
    "    resultados_f1 = []\n",
    "    resultados_parametros = []\n",
    "    \n",
    "    #dicionário das métricas\n",
    "    resultados_gerais = {}\n",
    "\n",
    "    for i in range(epocas):\n",
    "        # divisão os dados \n",
    "        X_train, X_test, y_train, y_test = procData.holdout(0.2)\n",
    "        #print(Counter(y_test))\n",
    "\n",
    "        # realizando o grid search para encontrar a melhor combinação entre o C, gamma e Kernel, \n",
    "        # considerando a acurácia (taxa de acerto)\n",
    "        # aqui o método GridSearchCV é configurado para subdividir os dados de treino em k_folds\n",
    "        \n",
    "        \n",
    "        clf = GradientBoostingClassifier(random_state=None)\n",
    "        grid_gb = GridSearchCV(clf, param_grid, cv=k_folds, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "        grid_gb.fit(X_train, y_train)\n",
    "\n",
    "        # Treinando do modelo com os melhores parametros encontrados\n",
    "        n_estimators_best = grid_gb.best_estimator_.n_estimators\n",
    "        learning_rate_best = grid_gb.best_estimator_.learning_rate\n",
    "        max_depth_best = grid_gb.best_estimator_.learning_rate\n",
    "\n",
    "        GB = GradientBoostingClassifier(random_state = None, n_estimators = n_estimators_best, learning_rate =  learning_rate_best, max_depth = max_depth_best)\n",
    "        GB.fit(X_train, y_train)\n",
    "\n",
    "        #testando o modelo\n",
    "        y_pred = GB.predict(X_test)\n",
    "        if exibir_matriz_confusao:\n",
    "            print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # calculado as metricas\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        precision = metrics.precision_score(y_test, y_pred)\n",
    "        recall = metrics.recall_score(y_test, y_pred)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # armazenando as métricas\n",
    "        resultados_accuracy.append(accuracy)\n",
    "        resultados_precision.append(precision)\n",
    "        resultados_recall.append(recall)\n",
    "        resultados_f1.append(f1_score)\n",
    "\n",
    "        best_parametros = \"n_estimators: \"+ str(n_estimators_best)+ \" | learning_rate: \"+ str(learning_rate_best)+ \" | max_depth: \"+ str(max_depth_best);\n",
    "        resultados_parametros.append(best_parametros)\n",
    "\n",
    "\n",
    "        if exibir_metricas:\n",
    "            print(\"Rodada: #\",i)\n",
    "            print(best_parametros)\n",
    "            print(\"Accuracy:\",accuracy)\n",
    "            print(\"Precision:\",precision)\n",
    "            print(\"Recall:\",recall)\n",
    "            print(\"F1-Score:\",f1_score)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            \n",
    "    resultados_gerais['accuracy'] = resultados_accuracy\n",
    "    resultados_gerais['precision'] = resultados_precision\n",
    "    resultados_gerais['recall'] = resultados_recall\n",
    "    resultados_gerais['f1'] = resultados_f1\n",
    "    \n",
    "    return resultados_gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabelaMetricas(nome_modelo, dict_metricas, rodadas=False, salvarResultados=True):\n",
    "        \n",
    "    print (\"============================================== \"+nome_modelo+\" =================================================\")\n",
    "    print (\"=================================== TABELA DE MÉTRICAS DO MODELO ===================================\")\n",
    "    \n",
    "    if(rodadas==False):\n",
    "        print (\"\\t Accuracy \\t|\\t Precision \\t|\\t Recall \\t|\\t F1-Score\")\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['accuracy'], axis=0), np.std(dict_metricas['accuracy'], axis=0)),end='       ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['precision'], axis=0), np.std(dict_metricas['precision'], axis=0)),end='    ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['recall'], axis=0), np.std(dict_metricas['recall'], axis=0)),end='       ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['f1'], axis=0), np.std(dict_metricas['f1'], axis=0)))\n",
    "        print (\"====================================================================================================\")\n",
    "        \n",
    "    if(salvarResultados):\n",
    "        # save to npy file\n",
    "        np.save('../resultados/resultados_'+nome_modelo+'.npy', dict_metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo os parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators  = [100, 110, 120]\n",
    "learning_rate = range (0.0, 1.0)\n",
    "max_depth     = np.arange(2,16,1)\n",
    "param_grid = {'n_estimators': n_estimators, 'learning_rate' : learning_rate, 'max_depth': max_depth}\n",
    "\n",
    "epocas = 50\n",
    "k_folds = 2\n",
    "exibir_matriz_confusao = True\n",
    "exibir_metricas = True\n",
    "salvarResultados = True\n",
    "rodadas=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando e obtendo as métricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24 10]\n",
      " [ 4 82]]\n",
      "Rodada: # 0\n",
      "n_estimators: 200 | learning_rate: 0.1\n",
      "Accuracy: 0.8833333333333333\n",
      "Precision: 0.8913043478260869\n",
      "Recall: 0.9534883720930233\n",
      "F1-Score: 0.9213483146067417\n",
      "\n",
      "\n",
      "[[28  8]\n",
      " [ 1 83]]\n",
      "Rodada: # 1\n",
      "n_estimators: 100 | learning_rate: 0.1\n",
      "Accuracy: 0.925\n",
      "Precision: 0.9120879120879121\n",
      "Recall: 0.9880952380952381\n",
      "F1-Score: 0.9485714285714287\n",
      "\n",
      "\n",
      "[[24  8]\n",
      " [ 3 85]]\n",
      "Rodada: # 2\n",
      "n_estimators: 150 | learning_rate: 0.1\n",
      "Accuracy: 0.9083333333333333\n",
      "Precision: 0.9139784946236559\n",
      "Recall: 0.9659090909090909\n",
      "F1-Score: 0.9392265193370166\n",
      "\n",
      "\n",
      "[[19 16]\n",
      " [ 3 82]]\n",
      "Rodada: # 3\n",
      "n_estimators: 100 | learning_rate: 0.5\n",
      "Accuracy: 0.8416666666666667\n",
      "Precision: 0.8367346938775511\n",
      "Recall: 0.9647058823529412\n",
      "F1-Score: 0.8961748633879782\n",
      "\n",
      "\n",
      "[[24 11]\n",
      " [ 1 84]]\n",
      "Rodada: # 4\n",
      "n_estimators: 150 | learning_rate: 0.1\n",
      "Accuracy: 0.9\n",
      "Precision: 0.8842105263157894\n",
      "Recall: 0.9882352941176471\n",
      "F1-Score: 0.9333333333333333\n",
      "\n",
      "\n",
      "============================================== GBDT =================================================\n",
      "=================================== TABELA DE MÉTRICAS DO MODELO ===================================\n",
      "\t Accuracy \t|\t Precision \t|\t Recall \t|\t F1-Score\n",
      "      0.89 +- 0.03             0.89 +- 0.03          0.97 +- 0.01             0.93 +- 0.02\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# treinando o modelo\n",
    "dict_metricas = treinamento_teste(epocas, k_folds, exibir_matriz_confusao, exibir_metricas)\n",
    "tabelaMetricas('GBDT',dict_metricas, rodadas, salvarResultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
