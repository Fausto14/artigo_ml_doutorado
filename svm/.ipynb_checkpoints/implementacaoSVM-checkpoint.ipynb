{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "from collections import Counter\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../Util'))\n",
    "from dados import ProcessarDados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "procData = ProcessarDados(\"../dataset/norm_bin_10_FEATURES_M17_CM6b_TH199.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcção que treina e testa o modelo armazenando as métricas\n",
    "# retorna um dicionário cotendo os valores das métricas de cada rodada\n",
    "def treinamento_teste(epocas = 10, k_folds = 5, exibir_matriz_confusao=False, exibir_metricas=False):\n",
    "    \n",
    "    #array para armazenar as das métricas de cada rodada\n",
    "    resultados_accuracy = []\n",
    "    resultados_precision = []\n",
    "    resultados_recall = []\n",
    "    resultados_f1 = []\n",
    "    resultados_parametros = []\n",
    "    \n",
    "    #dicionário das métricas\n",
    "    resultados_gerais = {}\n",
    "\n",
    "    for i in range(epocas):\n",
    "        # divisão os dados \n",
    "        X_train, X_test, y_train, y_test = procData.holdout(0.2)\n",
    "        #print(Counter(y_test))\n",
    "\n",
    "        # realizando o grid search para encontrar a melhor combinação entre o C, gamma e Kernel, \n",
    "        # considerando a acurácia (taxa de acerto)\n",
    "        # aqui o método GridSearchCV é configurado para subdividir os dados de treino em k_folds\n",
    "        clf = svm.SVC(random_state = 0)\n",
    "        grid_svm = GridSearchCV(clf, param_grid, cv=k_folds, scoring='accuracy', verbose=0)\n",
    "        grid_svm.fit(X_train, y_train)\n",
    "\n",
    "        # Treinando do modelo com os melhores parametros encontrados\n",
    "        C_best = grid_svm.best_estimator_.C\n",
    "        gamma_best = grid_svm.best_estimator_.gamma\n",
    "        kernel_best = grid_svm.best_estimator_.kernel\n",
    "\n",
    "        SVM = svm.SVC(random_state = 0, C = C_best, gamma = gamma_best, kernel = kernel_best)\n",
    "        SVM.fit(X_train, y_train)\n",
    "\n",
    "        #testando o modelo\n",
    "        y_pred = SVM.predict(X_test)\n",
    "        if exibir_matriz_confusao:\n",
    "            print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # calculado as metricas\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        precision = metrics.precision_score(y_test, y_pred)\n",
    "        recall = metrics.recall_score(y_test, y_pred)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # armazenando as métricas\n",
    "        resultados_accuracy.append(accuracy)\n",
    "        resultados_precision.append(precision)\n",
    "        resultados_recall.append(recall)\n",
    "        resultados_f1.append(f1_score)\n",
    "\n",
    "        best_parametros = \"C: \"+ str(C_best)+ \" | Gamma: \"+ str(gamma_best)+ \" | Kernel: \"+ str(kernel_best);\n",
    "        resultados_parametros.append(best_parametros)\n",
    "\n",
    "\n",
    "        if exibir_metricas:\n",
    "            print(\"Rodada: #\",i)\n",
    "            print(best_parametros)\n",
    "            print(\"Accuracy:\",accuracy)\n",
    "            print(\"Precision:\",precision)\n",
    "            print(\"Recall:\",recall)\n",
    "            print(\"F1-Score:\",f1_score)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            \n",
    "    resultados_gerais['accuracy'] = resultados_accuracy\n",
    "    resultados_gerais['precision'] = resultados_precision\n",
    "    resultados_gerais['recall'] = resultados_recall\n",
    "    resultados_gerais['f1'] = resultados_f1\n",
    "    \n",
    "    return resultados_gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabelaMetricas(nome_modelo, dict_metricas, rodadas=False, salvarResultados=True):\n",
    "        \n",
    "    print (\"============================================== \"+nome_modelo+\" =================================================\")\n",
    "    print (\"=================================== TABELA DE MÉTRICAS DO MODELO ===================================\")\n",
    "    \n",
    "    if(rodadas==False):\n",
    "        print (\"\\t Accuracy \\t|\\t Precision \\t|\\t Recall \\t|\\t F1-Score\")\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['accuracy'], axis=0), np.std(dict_metricas['accuracy'], axis=0)),end='       ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['precision'], axis=0), np.std(dict_metricas['precision'], axis=0)),end='    ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['recall'], axis=0), np.std(dict_metricas['recall'], axis=0)),end='       ')\n",
    "        print (\"      %.2f +- %.2f\" % (np.mean(dict_metricas['f1'], axis=0), np.std(dict_metricas['f1'], axis=0)))\n",
    "        print (\"====================================================================================================\")\n",
    "        \n",
    "    if(salvarResultados):\n",
    "        # save to npy file\n",
    "        save('resultados_svm.npy', dict_metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo os parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_range = [10, 20, 30, 40, 50]\n",
    "gammas = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "kernels = [\"rbf\"]\n",
    "param_grid = {'C': c_range, 'gamma': gammas, 'kernel': kernels}\n",
    "\n",
    "epocas = 5\n",
    "k_folds = 5\n",
    "exibir_matriz_confusao = True\n",
    "exibir_metricas = True\n",
    "salvarResultados = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando e obtendo as métricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25  8]\n",
      " [ 5 82]]\n",
      "Rodada: # 0\n",
      "C: 10 | Gamma: 0.04 | Kernel: rbf\n",
      "Accuracy: 0.8916666666666667\n",
      "Precision: 0.9111111111111111\n",
      "Recall: 0.9425287356321839\n",
      "F1-Score: 0.9265536723163842\n",
      "\n",
      "\n",
      "[[22 10]\n",
      " [ 6 82]]\n",
      "Rodada: # 1\n",
      "C: 50 | Gamma: 0.01 | Kernel: rbf\n",
      "Accuracy: 0.8666666666666667\n",
      "Precision: 0.8913043478260869\n",
      "Recall: 0.9318181818181818\n",
      "F1-Score: 0.9111111111111111\n",
      "\n",
      "\n",
      "[[15  6]\n",
      " [ 8 91]]\n",
      "Rodada: # 2\n",
      "C: 30 | Gamma: 0.04 | Kernel: rbf\n",
      "Accuracy: 0.8833333333333333\n",
      "Precision: 0.9381443298969072\n",
      "Recall: 0.9191919191919192\n",
      "F1-Score: 0.9285714285714285\n",
      "\n",
      "\n",
      "[[26  7]\n",
      " [ 3 84]]\n",
      "Rodada: # 3\n",
      "C: 10 | Gamma: 0.04 | Kernel: rbf\n",
      "Accuracy: 0.9166666666666666\n",
      "Precision: 0.9230769230769231\n",
      "Recall: 0.9655172413793104\n",
      "F1-Score: 0.9438202247191013\n",
      "\n",
      "\n",
      "[[18 16]\n",
      " [ 2 84]]\n",
      "Rodada: # 4\n",
      "C: 10 | Gamma: 0.02 | Kernel: rbf\n",
      "Accuracy: 0.85\n",
      "Precision: 0.84\n",
      "Recall: 0.9767441860465116\n",
      "F1-Score: 0.9032258064516129\n",
      "\n",
      "\n",
      "============================================== SVM =================================================\n",
      "=================================== TABELA DE MÉTRICAS DO MODELO ===================================\n"
     ]
    }
   ],
   "source": [
    "# treinando o modelo\n",
    "dict_metricas = treinamento_teste(epocas, k_folds, exibir_matriz_confusao, exibir_metricas)\n",
    "tabelaMetricas('SVM',dict_metricas, rodadas=False, salvarResultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
